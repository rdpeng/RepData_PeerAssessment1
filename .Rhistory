shiny::runApp('shiny-server/shiny_tweets2')
shiny::runApp('shiny-server/QualitativeAnalysisApp-R-master')
install.packages("pacman", lib="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
shiny::runApp('shiny-server/QualitativeAnalysisApp-R-master')
shiny::runApp('shiny-server/QualitativeAnalysisApp-R-master')
library(swirl)
install_from_swirl("Exploratory Data Analysis")
swirl()
install.packages(c("httr", "memoise", "nnet", "pbkrtest", "RMySQL"), lib="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
```{r xtable, results="asis"}
library(kernlab)
install.packages("kernlab", lib="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library(kernlab)
data(spam)
View(spam)
### Reproducible Research
## Concepts and Ideas
# Replication
# Reproducible Research: make analytic data and code available
# so that others may reproduce findings.
# Replication <- Reproducibility -> Nothing
## Research Pipeline
# Measured Data -> Analytic Data -> Computational Results
# Data/metadata, computer code, all the steps of computational analysis
## What do we need:
# Analytic data, analytic code, documentation of code and data,
# standard means of distribution.
## What are the players:
# Authors & Readers
## Literate(statistical) Programming
# An article is a stream of text and code
# 1. documentation language (Latex)
# 2. programming language (R)
# -> Sweave, knitr package
## Scriping your analysis
# like music, writing down score
# developing notations
## Structure of a Data Analysis
# Steps:
# 1. Define the question
# 2. Define the ideal data set
# Goal: descriptive(population), exploratory(random sample),
#       inferential(right population, randomly sampled),
#       predictive(training/test), causal(randomized study), mechanistic
# 3. Determine what data you can access
# Term of Use, UCI Spambase ML data
# 4. Obtain the data
# Raw data, reference the source, record the url and time accessed
# kernlab
# 5. Clean the data
# Reformating, subsampling
library(kernlab)
data(spam)
str(spam[, 1:5])
# 6. Exploratory data analysis
set.seed(3435)
trainIndicator <- rbinom(4601, size=1, prob=0.5)
table(trainIndicator)
trainSpam <- spam[trainIndicator==1, ]
testSpam <- spam[trainIndicator==0, ]
# summaries of the data, missing data, exploratory plots
names(trainSpam)
head(trainSpam)
table(trainSpam$type)
plot(trainSpam$capitalAve ~ trainSpam$type)
plot(log10(trainSpam$capitalAve + 1) ~ trainSpam$type)
# Relationships between predictors
plot(log10(trainSpam[, 1:4] + 1))
# Clustering
hCluster <- hclust(dist(t(trainSpam[, 1:57])))
plot(hCluster)
# New clustering
hCluster <- hclust(dist(t(log(trainSpam[, 1:55]))))
plot(hCluster)
# 7. Statistical prediction/modeling
trainSpam$numType <- as.numeric(trainSpam$type) - 1
costFunction <- function(x, y) sum(x != (y > 0.5))
cvError <- rep(NA, 55)
library(boot)
for(i in 1:55) {
lmFormula <- reformulate(names(trainSpam)[i], response="numType")
glmFit <- glm(lmFormula, family="binomial", data=trainSpam)
cvError[i] <- cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
}
# which predictor has min cv-error?
names(trainSpam)[which.min(cvError)]
# use the best model from the group
predictionModel <- glm(numType ~ charDollar, family="binomial", data=trainSpam)
# get predictions on the test set
predictionTest <- predict(predictionModel, testSpam)
predictedSpam <- rep("nonspam", dim(testSpam)[1])
# classify as 'spam' for those with prob > 0.5
predictedSpam[predictionModel$fitted > 0.5] <- "spam"
# classification table
table(predictedSpam, testSpam$type)
(77+457)/(1346+457+77+456) # error rate
# 8. Interpret results
# describes, correlates with/associated with, leads to/causes, predicts
# interpret coefficients
# 9. Challenge results
# Question, Data Source, Processing, Analysis, Conclusions
# Measures of uncertainty, choices of terms to include in models
# Alternative analysis methods
# 10. Synthesize/write up results
# Lead with the question, summarize the analyses into the story
# 11. Create reproducible code
## Organizing a Data Analysis
# Data: raw data(in analysis folder),
#       processed data(should be tidy)
# Figures: exploratory figures(don't need to be pretty),
#          final figures(clear, multiple panels)
# R code: raw/unused scripts(less commented),
#         final scripts(processing details, bigger commented blocks),
#         R Markdown files(text & R are integrated)
# Text: README files(url, description, date accessed),
#       Text of analysis/report(title, introduction, methods,
#                               results and conclusions)
setwd("~/Google Drive/Programmes/R/RepData_PeerAssessment1")
